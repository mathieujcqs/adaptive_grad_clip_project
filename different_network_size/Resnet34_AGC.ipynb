{"cells":[{"cell_type":"markdown","metadata":{"id":"hARQv_GHztHg"},"source":["#Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4805,"status":"ok","timestamp":1642886575051,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"v8bGqVmEzxcL","outputId":"59df9e0b-a6b6-45b7-cc99-184949f40564"},"outputs":[],"source":["#@title Print TF version and GPU stats\n","import tensorflow as tf\n","print('TensorFlow version:', tf.__version__)\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","   raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name), '', sep='\\n')\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4523,"status":"ok","timestamp":1642886579557,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"nwAEGJqzzzV0","outputId":"43f0d285-5c56-4259-9db6-1c8eabb884ac"},"outputs":[],"source":["from tensorflow.keras.datasets import cifar10\n","(train_X, train_y), (test_X, test_y) = cifar10.load_data()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1642886579558,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"vEXMzYtVz1R9"},"outputs":[],"source":["#We need to make onehot vectors out of the labels\n","from tensorflow.keras.utils import to_categorical\n","train_y = to_categorical(train_y,num_classes=10)\n","test_y = to_categorical(test_y,num_classes=10)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":360,"status":"ok","timestamp":1642886579905,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"y1TeIEK7z4x8"},"outputs":[],"source":["#@title Data augmentation\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","batch_size = 512\n","\n","# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1./255,\n","                                   rotation_range=20,\n","                                   width_shift_range=0.1,\n","                                   height_shift_range=0.1,\n","                                   zoom_range=0.4,\n","                                   brightness_range=(.5, 1.5),\n","                                   horizontal_flip=True,\n","                                   vertical_flip=True,\n","                                   fill_mode='nearest')\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Flow training images in batches using train_datagen generator\n","train_generator = train_datagen.flow(\n","                    train_X, train_y,\n","                    batch_size=batch_size,\n","                    shuffle=True)\n","\n","num_classes = train_generator.num_classes\n","\n","# Flow test images in batches using val_datagen generator\n","test_generator = test_datagen.flow(\n","                    test_X, test_y,\n","                    batch_size=batch_size,\n","                    shuffle=True)\n","\n","train_steps = len(train_generator)\n","test_steps = len(test_generator)"]},{"cell_type":"markdown","metadata":{"id":"xx9VzAsnzf4a"},"source":["#ResNet34 architecture definition"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1642886579906,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"oElxLQi5TcJK"},"outputs":[],"source":["#code for resnet34 (34-layer ResNet)\n","DEFAULT_FILTER_SIZE = 3\n","INIT_LR = 1e-3\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1642886579906,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"VwJC1dUMTgzS"},"outputs":[],"source":["def identity_block(input, f, filters, conv_num, step):\n","    \n","    # Names of layers\n","    conv_name_base = 'res_' + str(conv_num) + '_' + str(step)\n","    \n","    F1, F2 = filters\n","    \n","    # Save skip value\n","    x_skip = input\n","    \n","    # First component of main path\n","    x = layers.Conv2D(filters = F1, kernel_size = (f, f),\n","                      padding='same',\n","                      name = conv_name_base + '_a')(input)\n","\n","    x = layers.Activation('relu')(x)\n","    \n","    # Third component of main path \n","    x = layers.Conv2D(filters = F2, kernel_size = (f, f),\n","                      padding='same',\n","                      name = conv_name_base + '_b')(x)\n","\n","    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n","    x = layers.Add()([x, x_skip])\n","    x = layers.Activation('relu')(x)\n","    \n","    return x"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1642886579907,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"Yre5xc6LTmWF"},"outputs":[],"source":["def conv_block(input, f, filters, conv_num, step, s=2):\n","    \n","    # Names of layers\n","    conv_name_base = 'res_' + str(conv_num) + '_' + str(step)\n","    \n","    F1, F2 = filters\n","    \n","    # Save skip value\n","    x_skip = input\n","    \n","    # First layer of main path\n","    x = layers.Conv2D(filters=F1, kernel_size=(f, f), strides=(s,s),\n","                      padding='same',\n","                      name=conv_name_base + '_a')(input)\n","\n","    x = layers.Activation('relu')(x)\n","\n","    # Second layer of main path \n","    x = layers.Conv2D(filters = F2, kernel_size = (f, f),\n","                      strides=(1,1),\n","                      padding='same',\n","                      name = conv_name_base + '_b')(x)\n","\n","    # SKIP CONNECTION #\n","    x_skip = layers.Conv2D(filters=F2, kernel_size=(1, 1), strides=(s, s),\n","                    name=conv_name_base + '_1')(x_skip)\n","\n","    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n","    x = layers.Add()([x, x_skip])\n","    x = layers.Activation('relu')(x)\n","    \n","    return x"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1642886579908,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"oOCohXlQTqzv"},"outputs":[],"source":["def ResNet34(input_shape, classes):\n","  # Define the input as a tensor with shape input_shape\n","    x_input = layers.Input(input_shape)\n","    \n","    # Zero-Padding\n","    x = layers.ZeroPadding2D((3, 3))(x_input)\n","    \n","    # Conv 1\n","    x = layers.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer='he_normal')(x)\n","    x = layers.Activation('relu')(x)\n","    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n","\n","    # Conv 2\n","    x = conv_block(x, f=DEFAULT_FILTER_SIZE, filters=[64,64], conv_num=2, step=1, s=1)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[64,64], conv_num=2, step=2)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[64,64], conv_num=2, step=3)\n","\n","    # Conv 3\n","    x = conv_block(x, f=DEFAULT_FILTER_SIZE, filters=[128,128], conv_num=3, step=1, s=2)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[128,128], conv_num=3, step=2)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[128,128], conv_num=3, step=3)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[128,128], conv_num=3, step=4)\n","\n","    # Conv 4\n","    x = conv_block(x, f=DEFAULT_FILTER_SIZE, filters=[256,256], conv_num=4, step=1)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[256,256], conv_num=4, step=2)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[256,256], conv_num=4, step=3)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[256,256], conv_num=4, step=4)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[256,256], conv_num=4, step=5)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[256,256], conv_num=4, step=6)\n","\n","    # Conv 5\n","    x = conv_block(x, f=DEFAULT_FILTER_SIZE, filters=[512,512], conv_num=5, step=1)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[512,512], conv_num=5, step=2)\n","    x = identity_block(x, f=DEFAULT_FILTER_SIZE, filters=[512,512], conv_num=5, step=3)\n","\n","    #Pooling layer\n","    x = layers.AveragePooling2D(pool_size=(2,2), padding='same')(x)\n","\n","    # Output layer\n","    x = layers.Flatten()(x)\n","    output = layers.Dense(classes, activation='softmax',\n","                          name='fc' + str(classes),\n","                          kernel_initializer = 'he_normal')(x)\n","\n","    model = Model(x_input, output, name=\"ResNet_18\")\n","\n","    model.compile(loss='categorical_crossentropy',\n","                optimizer=Adam(learning_rate=INIT_LR),\n","                metrics=['accuracy'])\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"6MUYlBYdz-M8"},"source":["#AGC implementation"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1642886579908,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"xqAqoJhX0BSG"},"outputs":[],"source":["import tensorflow as tf\n","\n","def compute_norm(x, axis, keepdims):\n","    return tf.math.reduce_sum(x ** 2, axis=axis, keepdims=keepdims) ** 0.5\n","\n","def unitwise_norm(x):\n","    if len(x.get_shape()) <= 1:  # Scalars and vectors\n","        axis = None\n","        keepdims = False\n","    elif len(x.get_shape()) in [2, 3]:  # Linear layers of shape IO or multihead linear\n","        axis = 0\n","        keepdims = True\n","    elif len(x.get_shape()) == 4:  # Conv kernels of shape HWIO\n","        axis = [0, 1, 2,]\n","        keepdims = True\n","    else:\n","        raise ValueError(f\"Got a parameter with shape not in [1, 2, 4]! {x}\")\n","    return compute_norm(x, axis, keepdims)\n","\n","\n","def adaptive_clip_grad(parameters, gradients, clip_factor=0.01,\n","                       eps=1e-3):\n","    new_grads = []\n","    for (params, grads) in zip(parameters, gradients):\n","        p_norm = unitwise_norm(params)\n","        max_norm = tf.math.maximum(p_norm, eps) * clip_factor\n","        grad_norm = unitwise_norm(grads)\n","        clipped_grad = grads * (max_norm / tf.math.maximum(grad_norm, 1e-6))\n","        new_grad = tf.where(grad_norm < max_norm, grads, clipped_grad)\n","        new_grads.append(new_grad)\n","    return new_grads"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1642886579909,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"g631FS3j0CCf"},"outputs":[],"source":["import tensorflow as tf\n","\n","class ResNet34_AGC_Model(Model):\n","\n","  def __init__(self, inputshape, name, threshold=1e-3):\n","        super(ResNet34_AGC_Model, self).__init__()\n","        self.threshold = threshold\n","        self.resnet = ResNet34(input_shape=inputshape, classes=10)\n","\n","  #this bit of code is from keras API\n","  def train_step(self, data):\n","    # Unpack the data\n","    images, y = data\n","\n","    with tf.GradientTape() as tape:\n","        y_pred = self.resnet(images)  # Forward pass\n","        # Compute the loss value\n","        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","\n","    # Compute gradients\n","    trainable_vars = self.resnet.trainable_variables\n","    gradients = tape.gradient(loss, trainable_vars)\n","    # Update gradients\n","    updated_grad = adaptive_clip_grad(trainable_vars, gradients, self.threshold)\n","    # Update weights\n","    self.optimizer.apply_gradients(zip(updated_grad, trainable_vars))\n","    # Update metrics (includes the metric that tracks the loss)\n","    self.compiled_metrics.update_state(y, y_pred)\n","    # Return a dict mapping metric names to current value\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  def test_step(self, data):\n","    images, labels = data\n","    predictions = self.resnet(images, training=False)\n","    loss = self.compiled_loss(labels, predictions)\n","    self.compiled_metrics.update_state(labels, predictions)\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  def call(self, inputs):\n","    return self.resnet(inputs)"]},{"cell_type":"markdown","metadata":{"id":"-bZSZy5g0IvH"},"source":["#Model building and training"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":321,"status":"ok","timestamp":1642886580220,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"GL5cTZ5v0H52"},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","clipping_factor = 0.08 # As described in the base paper of this study\n","resnet34 = ResNet34_AGC_Model((32,32,3), threshold=0.08, name='resnet18_agc')\n","\n","resnet34.compile(loss='categorical_crossentropy',\n","            optimizer=Adam(learning_rate=1e-3),\n","            metrics=['accuracy'])"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1642886580221,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"2kTtXg7Q0QmC"},"outputs":[],"source":["#@title `plot_history()` definition\n","from matplotlib import pyplot as plt\n","\n","def plot_history(history):\n","  fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, dpi=150)\n","  ax1.plot(history.history['loss'], label='training')\n","  ax1.plot(history.history['val_loss'], label='validation')\n","  ax1.set_ylabel('Cross-Entropy Loss')\n","  ax1.set_yscale('log')\n","  if history.history.__contains__('lr'):\n","    ax1b = ax1.twinx()\n","    ax1b.plot(history.history['lr'], 'g-', linewidth=1)\n","    ax1b.set_yscale('log')\n","    ax1b.set_ylabel('Learning Rate', color='g')\n","\n","  ax2.plot(history.history['accuracy'], label='training')\n","  ax2.plot(history.history['val_accuracy'], label='validation')\n","  ax2.set_ylabel('Accuracy')\n","  ax2.set_xlabel('Epochs')\n","  ax2.legend()\n","  plt.show() "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3542920,"status":"ok","timestamp":1642890123134,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"1TrCfo240TDz","outputId":"d18a15d0-d70e-432b-a88e-75f870eeb11f"},"outputs":[],"source":["history = resnet34.fit(train_generator,\n","                        steps_per_epoch=train_steps,# trained with 512\n","                        epochs=100,\n","                        validation_data=test_generator,\n","                        validation_steps=test_steps\n","                       )\n","plot_history(history)"]}],"metadata":{"accelerator":"GPU","colab":{"name":"Resnet34_AGC.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
