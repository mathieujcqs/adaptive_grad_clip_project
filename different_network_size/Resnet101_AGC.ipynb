{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Resnet101_AGC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Data download and preparation"],"metadata":{"id":"-bqzSUU_pI_g"}},{"cell_type":"code","source":["#@title Print TF version and GPU stats\n","import tensorflow as tf\n","print('TensorFlow version:', tf.__version__)\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","   raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name), '', sep='\\n')\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cmaMqjOpULX","outputId":"2a6dee24-a774-4d0d-bdfc-563193c399ce","executionInfo":{"status":"ok","timestamp":1642716842507,"user_tz":-60,"elapsed":7097,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.7.0\n","Found GPU at: /device:GPU:0\n","\n","Thu Jan 20 22:14:02 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   53C    P0    28W /  70W |    264MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.datasets import cifar10\n","(train_X, train_y), (test_X, test_y) = cifar10.load_data()"],"metadata":{"id":"Q8kQIbf5qK5V","colab":{"base_uri":"https://localhost:8080/"},"outputId":"72c56bbf-491a-498a-984b-ad4997f015e8","executionInfo":{"status":"ok","timestamp":1642716847394,"user_tz":-60,"elapsed":4894,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 2s 0us/step\n","170508288/170498071 [==============================] - 2s 0us/step\n"]}]},{"cell_type":"code","source":["#We need to make onehot vectors out of the labels\n","from tensorflow.keras.utils import to_categorical\n","train_y = to_categorical(train_y,num_classes=10)\n","test_y = to_categorical(test_y,num_classes=10)"],"metadata":{"id":"KHDKf4SRqVML","executionInfo":{"status":"ok","timestamp":1642716847395,"user_tz":-60,"elapsed":12,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#@title Prepare Data Generators\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","batch_size = 512\n","\n","# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1./255,\n","                                   rotation_range=20,\n","                                   width_shift_range=0.1,\n","                                   height_shift_range=0.1,\n","                                   zoom_range=0.4,\n","                                   brightness_range=(.5, 1.5),\n","                                   horizontal_flip=True,\n","                                   vertical_flip=True,\n","                                   fill_mode='nearest')\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Flow training images in batches using train_datagen generator\n","train_generator = train_datagen.flow(\n","                    train_X, train_y,\n","                    batch_size=batch_size,\n","                    shuffle=True)\n","\n","num_classes = 10\n","\n","# Flow test images in batches using val_datagen generator\n","test_generator = test_datagen.flow(\n","                    test_X, test_y,\n","                    batch_size=batch_size,\n","                    shuffle=True)\n","\n","train_steps = len(train_generator)\n","test_steps = len(test_generator)"],"metadata":{"id":"MEgpj9tEqd1w","executionInfo":{"status":"ok","timestamp":1642716847396,"user_tz":-60,"elapsed":11,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#@title `plot_history()` definition\n","from matplotlib import pyplot as plt\n","\n","def plot_history(history):\n","  fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, dpi=150)\n","  ax1.plot(history.history['loss'], label='training')\n","  ax1.plot(history.history['val_loss'], label='validation')\n","  ax1.set_ylabel('Cross-Entropy Loss')\n","  ax1.set_yscale('log')\n","  if history.history.__contains__('lr'):\n","    ax1b = ax1.twinx()\n","    ax1b.plot(history.history['lr'], 'g-', linewidth=1)\n","    ax1b.set_yscale('log')\n","    ax1b.set_ylabel('Learning Rate', color='g')\n","\n","  ax2.plot(history.history['accuracy'], label='training')\n","  ax2.plot(history.history['val_accuracy'], label='validation')\n","  ax2.set_ylabel('Accuracy')\n","  ax2.set_xlabel('Epochs')\n","  ax2.legend()\n","  plt.show() "],"metadata":{"id":"cqXltJE0tTg_","executionInfo":{"status":"ok","timestamp":1642716847397,"user_tz":-60,"elapsed":10,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Defining AGC model"],"metadata":{"id":"7ayKGD7Hq7xe"}},{"cell_type":"markdown","source":["For the implementation of the adaptive gradient clipping we use this following github [repo](https://github.com/sayakpaul/Adaptive-Gradient-Clipping/blob/main/agc.py)"],"metadata":{"id":"3NUeuO_OrNFZ"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def compute_norm(x, axis, keepdims):\n","    return tf.math.reduce_sum(x ** 2, axis=axis, keepdims=keepdims) ** 0.5\n","\n","def unitwise_norm(x):\n","    if len(x.get_shape()) <= 1:  # Scalars and vectors\n","        axis = None\n","        keepdims = False\n","    elif len(x.get_shape()) in [2, 3]:  # Linear layers of shape IO or multihead linear\n","        axis = 0\n","        keepdims = True\n","    elif len(x.get_shape()) == 4:  # Conv kernels of shape HWIO\n","        axis = [0, 1, 2,]\n","        keepdims = True\n","    else:\n","        raise ValueError(f\"Got a parameter with shape not in [1, 2, 4]! {x}\")\n","    return compute_norm(x, axis, keepdims)\n","\n","\n","def adaptive_clip_grad(parameters, gradients, clip_factor=0.01,\n","                       eps=1e-3):\n","    new_grads = []\n","    for (params, grads) in zip(parameters, gradients):\n","        p_norm = unitwise_norm(params)\n","        max_norm = tf.math.maximum(p_norm, eps) * clip_factor\n","        grad_norm = unitwise_norm(grads)\n","        clipped_grad = grads * (max_norm / tf.math.maximum(grad_norm, 1e-6))\n","        new_grad = tf.where(grad_norm < max_norm, grads, clipped_grad)\n","        new_grads.append(new_grad)\n","    return new_grads"],"metadata":{"id":"VtYcO2tTbMtk","executionInfo":{"status":"ok","timestamp":1642716847397,"user_tz":-60,"elapsed":9,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Resnet 101 architecture without batchnorm"],"metadata":{"id":"hofKI_b0rKOe"}},{"cell_type":"code","source":["\"\"\"ResNet152 model for Keras.\n","# Reference:\n","- [Deep Residual Learning for Image Recognition](\n","    https://arxiv.org/abs/1512.03385) (CVPR 2016 Best Paper Award)\n","Adapted from code contributed by BigMoyan.\n","\"\"\"\n","\n","import os\n","import warnings\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","\n","\n","def identity_block(input_tensor, kernel_size, filters, stage, block):\n","    \"\"\"The identity block is the block that has no conv layer at shortcut.\n","    # Arguments\n","        input_tensor: input tensor\n","        kernel_size: default 3, the kernel size of\n","            middle conv layer at main path\n","        filters: list of integers, the filters of 3 conv layer at main path\n","        stage: integer, current stage label, used for generating layer names\n","        block: 'a','b'..., current block label, used for generating layer names\n","    # Returns\n","        Output tensor for the block.\n","    \"\"\"\n","    filters1, filters2, filters3 = filters\n","\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","\n","\n","    x = layers.Conv2D(filters1, (1, 1),\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2a')(input_tensor)\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters2, kernel_size,\n","                      padding='same',\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2b')(x)\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters3, (1, 1),\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2c')(x)\n","\n","    x = layers.add([x, input_tensor])\n","    x = layers.Activation('relu')(x)\n","    return x\n","\n","\n","def conv_block(input_tensor,\n","               kernel_size,\n","               filters,\n","               stage,\n","               block,\n","               strides=(2, 2)):\n","    \"\"\"A block that has a conv layer at shortcut.\n","    # Arguments\n","        input_tensor: input tensor\n","        kernel_size: default 3, the kernel size of\n","            middle conv layer at main path\n","        filters: list of integers, the filters of 3 conv layer at main path\n","        stage: integer, current stage label, used for generating layer names\n","        block: 'a','b'..., current block label, used for generating layer names\n","        strides: Strides for the first conv layer in the block.\n","    # Returns\n","        Output tensor for the block.\n","    Note that from stage 3,\n","    the first conv layer at main path is with strides=(2, 2)\n","    And the shortcut should have strides=(2, 2) as well\n","    \"\"\"\n","    filters1, filters2, filters3 = filters\n","\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","\n","    x = layers.Conv2D(filters1, (1, 1), strides=strides,\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2a')(input_tensor)\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters2, kernel_size, padding='same',\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2b')(x)\n","\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters3, (1, 1),\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2c')(x)\n","\n","    shortcut = layers.Conv2D(filters3, (1, 1), strides=strides,\n","                             kernel_initializer='he_normal',\n","                             name=conv_name_base + '1')(input_tensor)\n","\n","    x = layers.add([x, shortcut])\n","    x = layers.Activation('relu')(x)\n","    return x\n","\n","\n","def ResNet101(input_shape=None,\n","             classes=10):\n","    \"\"\"Instantiates the ResNet50 architecture.\n","    # Arguments\n","        input_shape: optional shape tuple, only to be specified\n","            if `include_top` is False (otherwise the input shape\n","            has to be `(224, 224, 3)` (with `channels_last` data format)\n","            or `(3, 224, 224)` (with `channels_first` data format).\n","            It should have exactly 3 inputs channels,\n","            and width and height should be no smaller than 32.\n","            E.g. `(200, 200, 3)` would be one valid value.\n","        classes: optional number of classes to classify images\n","            into, only to be specified if `include_top` is True, and\n","            if no `weights` argument is specified.\n","    # Returns\n","        A Keras model instance.\n","    \"\"\"\n","    \n","    img_input = layers.Input(shape=input_shape)\n","\n","    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n","    x = layers.Conv2D(64, (7, 7),\n","                      strides=(2, 2),\n","                      padding='valid',\n","                      kernel_initializer='he_normal',\n","                      name='conv1')(x)\n","\n","    x = layers.Activation('relu')(x)\n","    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n","    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n","\n","    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n","    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n","    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n","\n","    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n","    for i in range(1,4):\n","      x = identity_block(x, 3, [128, 128, 512], stage=3, block=f\"b_{i}\")\n","\n","    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n","    for i in range(1,23):\n","      x = identity_block(x, 3, [256, 256, 1024], stage=4, block=f\"b_{i}\")\n","\n","    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n","    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n","    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n","\n","    \n","    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n","    x = layers.Dense(classes, activation='softmax', name='fc10')(x)\n","    \n","    model = Model(img_input, x)\n","\n","    return model"],"metadata":{"id":"Pn_A9v9BhdOR","executionInfo":{"status":"ok","timestamp":1642716847745,"user_tz":-60,"elapsed":356,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"1S65Wb-RZhmu","executionInfo":{"status":"ok","timestamp":1642716847746,"user_tz":-60,"elapsed":7,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"outputs":[],"source":["import tensorflow as tf\n","\n","class ResNet101_AGC_Model(Model):\n","\n","  def __init__(self, inputshape, name, threshold=1e-3):\n","        super(ResNet101_AGC_Model, self).__init__()\n","        self.threshold = threshold\n","        self.resnet = ResNet101(input_shape=inputshape, classes=10)\n","\n","  #this bit of code is from keras API\n","  def train_step(self, data):\n","    # Unpack the data\n","    images, y = data\n","\n","    with tf.GradientTape() as tape:\n","        y_pred = self.resnet(images)  # Forward pass\n","        # Compute the loss value\n","        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","\n","    # Compute gradients\n","    trainable_vars = self.resnet.trainable_variables\n","    gradients = tape.gradient(loss, trainable_vars)\n","    # Update gradients\n","    updated_grad = adaptive_clip_grad(trainable_vars, gradients, self.threshold)\n","    # Update weights\n","    self.optimizer.apply_gradients(zip(updated_grad, trainable_vars))\n","    # Update metrics (includes the metric that tracks the loss)\n","    self.compiled_metrics.update_state(y, y_pred)\n","    # Return a dict mapping metric names to current value\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  def test_step(self, data):\n","    images, labels = data\n","    predictions = self.resnet(images, training=False)\n","    loss = self.compiled_loss(labels, predictions)\n","    self.compiled_metrics.update_state(labels, predictions)\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  def call(self, inputs):\n","    return self.resnet(inputs)"]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam\n","clipping_factor = 0.08 # As described in the base paper of this study\n","resnet101 = ResNet101_AGC_Model((32,32,3), threshold=0.08, name='resnet152_agc')\n","\n","resnet101.compile(loss='categorical_crossentropy',\n","            optimizer=Adam(learning_rate=1e-3),\n","            metrics=['accuracy'])"],"metadata":{"id":"2WzmBaw3jtdp","executionInfo":{"status":"ok","timestamp":1642716848942,"user_tz":-60,"elapsed":1200,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["history = resnet101.fit(train_generator,\n","                        steps_per_epoch=train_steps,# trained with 512\n","                        epochs=100,\n","                        validation_data=test_generator,\n","                        validation_steps=test_steps\n","                       )\n","plot_history(history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ONoiY8lhpG3z","outputId":"e2ecdfde-7242-43b5-9451-9b8534bf9b97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","98/98 [==============================] - 76s 576ms/step - loss: 442.8994 - accuracy: 0.2160 - val_loss: 1.7924 - val_accuracy: 0.3411\n","Epoch 2/100\n","98/98 [==============================] - 51s 514ms/step - loss: 1.8224 - accuracy: 0.3257 - val_loss: 1.7022 - val_accuracy: 0.3645\n","Epoch 3/100\n","98/98 [==============================] - 51s 516ms/step - loss: 1.7041 - accuracy: 0.3772 - val_loss: 1.5865 - val_accuracy: 0.4241\n","Epoch 4/100\n","98/98 [==============================] - 51s 516ms/step - loss: 1.6507 - accuracy: 0.4006 - val_loss: 1.4863 - val_accuracy: 0.4584\n","Epoch 5/100\n","98/98 [==============================] - 51s 516ms/step - loss: 1.5946 - accuracy: 0.4221 - val_loss: 1.4378 - val_accuracy: 0.4720\n","Epoch 6/100\n","98/98 [==============================] - 50s 513ms/step - loss: 1.5618 - accuracy: 0.4353 - val_loss: 1.4367 - val_accuracy: 0.4786\n","Epoch 7/100\n","98/98 [==============================] - 51s 515ms/step - loss: 1.5279 - accuracy: 0.4521 - val_loss: 1.3734 - val_accuracy: 0.5039\n","Epoch 8/100\n","98/98 [==============================] - 52s 524ms/step - loss: 1.4912 - accuracy: 0.4623 - val_loss: 1.3309 - val_accuracy: 0.5196\n","Epoch 9/100\n","98/98 [==============================] - 50s 507ms/step - loss: 1.4787 - accuracy: 0.4668 - val_loss: 1.3576 - val_accuracy: 0.5043\n","Epoch 10/100\n","98/98 [==============================] - 49s 501ms/step - loss: 1.4428 - accuracy: 0.4827 - val_loss: 1.3188 - val_accuracy: 0.5277\n","Epoch 11/100\n","98/98 [==============================] - 50s 506ms/step - loss: 1.4174 - accuracy: 0.4907 - val_loss: 1.2570 - val_accuracy: 0.5499\n","Epoch 12/100\n","98/98 [==============================] - 49s 499ms/step - loss: 1.4127 - accuracy: 0.4915 - val_loss: 1.2317 - val_accuracy: 0.5508\n","Epoch 13/100\n","98/98 [==============================] - 49s 495ms/step - loss: 1.3790 - accuracy: 0.5045 - val_loss: 1.2443 - val_accuracy: 0.5514\n","Epoch 14/100\n","98/98 [==============================] - 49s 498ms/step - loss: 1.3578 - accuracy: 0.5139 - val_loss: 1.2028 - val_accuracy: 0.5653\n","Epoch 15/100\n","98/98 [==============================] - 49s 498ms/step - loss: 1.3522 - accuracy: 0.5157 - val_loss: 1.2490 - val_accuracy: 0.5486\n","Epoch 16/100\n","98/98 [==============================] - 49s 500ms/step - loss: 1.3279 - accuracy: 0.5259 - val_loss: 1.2677 - val_accuracy: 0.5412\n","Epoch 17/100\n","98/98 [==============================] - 49s 497ms/step - loss: 1.3082 - accuracy: 0.5329 - val_loss: 1.2309 - val_accuracy: 0.5544\n","Epoch 18/100\n","98/98 [==============================] - 49s 495ms/step - loss: 1.2978 - accuracy: 0.5350 - val_loss: 1.1515 - val_accuracy: 0.5884\n","Epoch 19/100\n","98/98 [==============================] - 49s 500ms/step - loss: 1.2826 - accuracy: 0.5403 - val_loss: 1.1464 - val_accuracy: 0.5890\n","Epoch 20/100\n","98/98 [==============================] - 49s 499ms/step - loss: 1.2613 - accuracy: 0.5493 - val_loss: 1.1649 - val_accuracy: 0.5827\n","Epoch 21/100\n","98/98 [==============================] - 49s 493ms/step - loss: 1.2596 - accuracy: 0.5515 - val_loss: 1.2081 - val_accuracy: 0.5616\n","Epoch 22/100\n","98/98 [==============================] - 49s 498ms/step - loss: 1.2422 - accuracy: 0.5606 - val_loss: 1.1340 - val_accuracy: 0.5869\n","Epoch 23/100\n","98/98 [==============================] - 49s 493ms/step - loss: 1.2249 - accuracy: 0.5630 - val_loss: 1.1493 - val_accuracy: 0.5903\n","Epoch 24/100\n","98/98 [==============================] - 49s 494ms/step - loss: 1.2298 - accuracy: 0.5623 - val_loss: 1.1297 - val_accuracy: 0.5959\n","Epoch 25/100\n","98/98 [==============================] - 48s 493ms/step - loss: 1.2220 - accuracy: 0.5635 - val_loss: 1.1778 - val_accuracy: 0.5782\n","Epoch 26/100\n","98/98 [==============================] - 49s 501ms/step - loss: 1.1981 - accuracy: 0.5716 - val_loss: 1.1428 - val_accuracy: 0.5917\n","Epoch 27/100\n","98/98 [==============================] - 49s 497ms/step - loss: 1.1910 - accuracy: 0.5750 - val_loss: 1.1226 - val_accuracy: 0.6039\n","Epoch 28/100\n","98/98 [==============================] - 48s 490ms/step - loss: 1.1751 - accuracy: 0.5793 - val_loss: 1.0438 - val_accuracy: 0.6327\n","Epoch 29/100\n","98/98 [==============================] - 48s 493ms/step - loss: 1.1567 - accuracy: 0.5875 - val_loss: 1.0591 - val_accuracy: 0.6246\n","Epoch 30/100\n","98/98 [==============================] - 48s 491ms/step - loss: 1.1614 - accuracy: 0.5872 - val_loss: 1.0175 - val_accuracy: 0.6350\n","Epoch 31/100\n","56/98 [================>.............] - ETA: 31s - loss: 1.1594 - accuracy: 0.5895"]}]}]}