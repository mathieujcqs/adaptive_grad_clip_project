{"cells":[{"cell_type":"markdown","metadata":{"id":"-bqzSUU_pI_g"},"source":["# Data download and preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7097,"status":"ok","timestamp":1642716842507,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"5cmaMqjOpULX","outputId":"2a6dee24-a774-4d0d-bdfc-563193c399ce"},"outputs":[],"source":["#@title Print TF version and GPU stats\n","import tensorflow as tf\n","print('TensorFlow version:', tf.__version__)\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","   raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name), '', sep='\\n')\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4894,"status":"ok","timestamp":1642716847394,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"Q8kQIbf5qK5V","outputId":"72c56bbf-491a-498a-984b-ad4997f015e8"},"outputs":[],"source":["from tensorflow.keras.datasets import cifar10\n","(train_X, train_y), (test_X, test_y) = cifar10.load_data()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1642716847395,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"KHDKf4SRqVML"},"outputs":[],"source":["#We need to make onehot vectors out of the labels\n","from tensorflow.keras.utils import to_categorical\n","train_y = to_categorical(train_y,num_classes=10)\n","test_y = to_categorical(test_y,num_classes=10)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1642716847396,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"MEgpj9tEqd1w"},"outputs":[],"source":["#@title Prepare Data Generators\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","batch_size = 512\n","\n","# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1./255,\n","                                   rotation_range=20,\n","                                   width_shift_range=0.1,\n","                                   height_shift_range=0.1,\n","                                   zoom_range=0.4,\n","                                   brightness_range=(.5, 1.5),\n","                                   horizontal_flip=True,\n","                                   vertical_flip=True,\n","                                   fill_mode='nearest')\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Flow training images in batches using train_datagen generator\n","train_generator = train_datagen.flow(\n","                    train_X, train_y,\n","                    batch_size=batch_size,\n","                    shuffle=True)\n","\n","num_classes = train_generator.num_classes\n","\n","# Flow test images in batches using val_datagen generator\n","test_generator = test_datagen.flow(\n","                    test_X, test_y,\n","                    batch_size=batch_size,\n","                    shuffle=True)\n","\n","train_steps = len(train_generator)\n","test_steps = len(test_generator)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1642716847397,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"cqXltJE0tTg_"},"outputs":[],"source":["#@title `plot_history()` definition\n","from matplotlib import pyplot as plt\n","\n","def plot_history(history):\n","  fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, dpi=150)\n","  ax1.plot(history.history['loss'], label='training')\n","  ax1.plot(history.history['val_loss'], label='validation')\n","  ax1.set_ylabel('Cross-Entropy Loss')\n","  ax1.set_yscale('log')\n","  if history.history.__contains__('lr'):\n","    ax1b = ax1.twinx()\n","    ax1b.plot(history.history['lr'], 'g-', linewidth=1)\n","    ax1b.set_yscale('log')\n","    ax1b.set_ylabel('Learning Rate', color='g')\n","\n","  ax2.plot(history.history['accuracy'], label='training')\n","  ax2.plot(history.history['val_accuracy'], label='validation')\n","  ax2.set_ylabel('Accuracy')\n","  ax2.set_xlabel('Epochs')\n","  ax2.legend()\n","  plt.show() "]},{"cell_type":"markdown","metadata":{"id":"7ayKGD7Hq7xe"},"source":["# Defining AGC model"]},{"cell_type":"markdown","metadata":{"id":"3NUeuO_OrNFZ"},"source":["For the implementation of the adaptive gradient clipping we use this following github [repo](https://github.com/sayakpaul/Adaptive-Gradient-Clipping/blob/main/agc.py)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1642716847397,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"VtYcO2tTbMtk"},"outputs":[],"source":["import tensorflow as tf\n","\n","def compute_norm(x, axis, keepdims):\n","    return tf.math.reduce_sum(x ** 2, axis=axis, keepdims=keepdims) ** 0.5\n","\n","def unitwise_norm(x):\n","    if len(x.get_shape()) <= 1:  # Scalars and vectors\n","        axis = None\n","        keepdims = False\n","    elif len(x.get_shape()) in [2, 3]:  # Linear layers of shape IO or multihead linear\n","        axis = 0\n","        keepdims = True\n","    elif len(x.get_shape()) == 4:  # Conv kernels of shape HWIO\n","        axis = [0, 1, 2,]\n","        keepdims = True\n","    else:\n","        raise ValueError(f\"Got a parameter with shape not in [1, 2, 4]! {x}\")\n","    return compute_norm(x, axis, keepdims)\n","\n","\n","def adaptive_clip_grad(parameters, gradients, clip_factor=0.01,\n","                       eps=1e-3):\n","    new_grads = []\n","    for (params, grads) in zip(parameters, gradients):\n","        p_norm = unitwise_norm(params)\n","        max_norm = tf.math.maximum(p_norm, eps) * clip_factor\n","        grad_norm = unitwise_norm(grads)\n","        clipped_grad = grads * (max_norm / tf.math.maximum(grad_norm, 1e-6))\n","        new_grad = tf.where(grad_norm < max_norm, grads, clipped_grad)\n","        new_grads.append(new_grad)\n","    return new_grads"]},{"cell_type":"markdown","metadata":{"id":"hofKI_b0rKOe"},"source":["# Resnet 101 architecture without batchnorm"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":356,"status":"ok","timestamp":1642716847745,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"Pn_A9v9BhdOR"},"outputs":[],"source":["\"\"\"ResNet152 model for Keras.\n","# Reference:\n","- [Deep Residual Learning for Image Recognition](\n","    https://arxiv.org/abs/1512.03385) (CVPR 2016 Best Paper Award)\n","Adapted from code contributed by BigMoyan.\n","\"\"\"\n","\n","import os\n","import warnings\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","\n","\n","def identity_block(input_tensor, kernel_size, filters, stage, block):\n","    \"\"\"The identity block is the block that has no conv layer at shortcut.\n","    # Arguments\n","        input_tensor: input tensor\n","        kernel_size: default 3, the kernel size of\n","            middle conv layer at main path\n","        filters: list of integers, the filters of 3 conv layer at main path\n","        stage: integer, current stage label, used for generating layer names\n","        block: 'a','b'..., current block label, used for generating layer names\n","    # Returns\n","        Output tensor for the block.\n","    \"\"\"\n","    filters1, filters2, filters3 = filters\n","\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","\n","\n","    x = layers.Conv2D(filters1, (1, 1),\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2a')(input_tensor)\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters2, kernel_size,\n","                      padding='same',\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2b')(x)\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters3, (1, 1),\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2c')(x)\n","\n","    x = layers.add([x, input_tensor])\n","    x = layers.Activation('relu')(x)\n","    return x\n","\n","\n","def conv_block(input_tensor,\n","               kernel_size,\n","               filters,\n","               stage,\n","               block,\n","               strides=(2, 2)):\n","    \"\"\"A block that has a conv layer at shortcut.\n","    # Arguments\n","        input_tensor: input tensor\n","        kernel_size: default 3, the kernel size of\n","            middle conv layer at main path\n","        filters: list of integers, the filters of 3 conv layer at main path\n","        stage: integer, current stage label, used for generating layer names\n","        block: 'a','b'..., current block label, used for generating layer names\n","        strides: Strides for the first conv layer in the block.\n","    # Returns\n","        Output tensor for the block.\n","    Note that from stage 3,\n","    the first conv layer at main path is with strides=(2, 2)\n","    And the shortcut should have strides=(2, 2) as well\n","    \"\"\"\n","    filters1, filters2, filters3 = filters\n","\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","\n","    x = layers.Conv2D(filters1, (1, 1), strides=strides,\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2a')(input_tensor)\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters2, kernel_size, padding='same',\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2b')(x)\n","\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters3, (1, 1),\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2c')(x)\n","\n","    shortcut = layers.Conv2D(filters3, (1, 1), strides=strides,\n","                             kernel_initializer='he_normal',\n","                             name=conv_name_base + '1')(input_tensor)\n","\n","    x = layers.add([x, shortcut])\n","    x = layers.Activation('relu')(x)\n","    return x\n","\n","\n","def ResNet101(input_shape=None,\n","             classes=10):\n","    \"\"\"Instantiates the ResNet50 architecture.\n","    # Arguments\n","        input_shape: optional shape tuple, only to be specified\n","            if `include_top` is False (otherwise the input shape\n","            has to be `(224, 224, 3)` (with `channels_last` data format)\n","            or `(3, 224, 224)` (with `channels_first` data format).\n","            It should have exactly 3 inputs channels,\n","            and width and height should be no smaller than 32.\n","            E.g. `(200, 200, 3)` would be one valid value.\n","        classes: optional number of classes to classify images\n","            into, only to be specified if `include_top` is True, and\n","            if no `weights` argument is specified.\n","    # Returns\n","        A Keras model instance.\n","    \"\"\"\n","    \n","    img_input = layers.Input(shape=input_shape)\n","\n","    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n","    x = layers.Conv2D(64, (7, 7),\n","                      strides=(2, 2),\n","                      padding='valid',\n","                      kernel_initializer='he_normal',\n","                      name='conv1')(x)\n","\n","    x = layers.Activation('relu')(x)\n","    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n","    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n","\n","    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n","    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n","    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n","\n","    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n","    for i in range(1,4):\n","      x = identity_block(x, 3, [128, 128, 512], stage=3, block=f\"b_{i}\")\n","\n","    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n","    for i in range(1,23):\n","      x = identity_block(x, 3, [256, 256, 1024], stage=4, block=f\"b_{i}\")\n","\n","    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n","    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n","    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n","\n","    \n","    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n","    x = layers.Dense(classes, activation='softmax', name='fc10')(x)\n","    \n","    model = Model(img_input, x)\n","\n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1642716847746,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"1S65Wb-RZhmu"},"outputs":[],"source":["import tensorflow as tf\n","\n","class ResNet101_AGC_Model(Model):\n","\n","  def __init__(self, inputshape, name, threshold=1e-3):\n","        super(ResNet101_AGC_Model, self).__init__()\n","        self.threshold = threshold\n","        self.resnet = ResNet101(input_shape=inputshape, classes=10)\n","\n","  #this bit of code is from keras API\n","  def train_step(self, data):\n","    # Unpack the data\n","    images, y = data\n","\n","    with tf.GradientTape() as tape:\n","        y_pred = self.resnet(images)  # Forward pass\n","        # Compute the loss value\n","        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","\n","    # Compute gradients\n","    trainable_vars = self.resnet.trainable_variables\n","    gradients = tape.gradient(loss, trainable_vars)\n","    # Update gradients\n","    updated_grad = adaptive_clip_grad(trainable_vars, gradients, self.threshold)\n","    # Update weights\n","    self.optimizer.apply_gradients(zip(updated_grad, trainable_vars))\n","    # Update metrics (includes the metric that tracks the loss)\n","    self.compiled_metrics.update_state(y, y_pred)\n","    # Return a dict mapping metric names to current value\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  def test_step(self, data):\n","    images, labels = data\n","    predictions = self.resnet(images, training=False)\n","    loss = self.compiled_loss(labels, predictions)\n","    self.compiled_metrics.update_state(labels, predictions)\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  def call(self, inputs):\n","    return self.resnet(inputs)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1200,"status":"ok","timestamp":1642716848942,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"},"user_tz":-60},"id":"2WzmBaw3jtdp"},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","clipping_factor = 0.08 # As described in the base paper of this study\n","resnet101 = ResNet101_AGC_Model((32,32,3), threshold=0.08, name='resnet152_agc')\n","\n","resnet101.compile(loss='categorical_crossentropy',\n","            optimizer=Adam(learning_rate=1e-3),\n","            metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ONoiY8lhpG3z","outputId":"e2ecdfde-7242-43b5-9451-9b8534bf9b97"},"outputs":[],"source":["history = resnet101.fit(train_generator,\n","                        steps_per_epoch=train_steps,# trained with 512\n","                        epochs=100,\n","                        validation_data=test_generator,\n","                        validation_steps=test_steps\n","                       )\n","plot_history(history)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Resnet101_AGC.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
