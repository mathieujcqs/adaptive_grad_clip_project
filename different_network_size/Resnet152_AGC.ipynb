{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Resnet152_AGC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Data download and preparation"],"metadata":{"id":"-bqzSUU_pI_g"}},{"cell_type":"code","source":["#@title Print TF version and GPU stats\n","import tensorflow as tf\n","print('TensorFlow version:', tf.__version__)\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","   raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name), '', sep='\\n')\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cmaMqjOpULX","outputId":"248a4081-6e4f-4a4d-ec4c-4e9dc4930adb","executionInfo":{"status":"ok","timestamp":1642715710371,"user_tz":-60,"elapsed":7421,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.7.0\n","Found GPU at: /device:GPU:0\n","\n","Thu Jan 20 21:55:10 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P0    28W /  70W |    264MiB / 15109MiB |      1%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.datasets import cifar10\n","(train_X, train_y), (test_X, test_y) = cifar10.load_data()"],"metadata":{"id":"Q8kQIbf5qK5V","executionInfo":{"status":"ok","timestamp":1642715711643,"user_tz":-60,"elapsed":1429,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#We need to make onehot vectors out of the labels\n","from tensorflow.keras.utils import to_categorical\n","train_y = to_categorical(train_y,num_classes=10)\n","test_y = to_categorical(test_y,num_classes=10)"],"metadata":{"id":"KHDKf4SRqVML","executionInfo":{"status":"ok","timestamp":1642715711645,"user_tz":-60,"elapsed":25,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#@title Prepare Data Generators\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","batch_size = 512\n","\n","# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1./255,\n","                                   rotation_range=20,\n","                                   width_shift_range=0.1,\n","                                   height_shift_range=0.1,\n","                                   zoom_range=0.4,\n","                                   brightness_range=(.5, 1.5),\n","                                   horizontal_flip=True,\n","                                   vertical_flip=True,\n","                                   fill_mode='nearest')\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Flow training images in batches using train_datagen generator\n","train_generator = train_datagen.flow(\n","                    train_X, train_y,\n","                    batch_size=batch_size,\n","                    shuffle=True)\n","\n","num_classes = 10\n","\n","# Flow test images in batches using val_datagen generator\n","test_generator = test_datagen.flow(\n","                    test_X, test_y,\n","                    batch_size=batch_size,\n","                    shuffle=True)\n","\n","train_steps = len(train_generator)\n","test_steps = len(test_generator)"],"metadata":{"id":"MEgpj9tEqd1w","executionInfo":{"status":"ok","timestamp":1642715711647,"user_tz":-60,"elapsed":21,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#@title `plot_history()` definition\n","from matplotlib import pyplot as plt\n","\n","def plot_history(history):\n","  fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, dpi=150)\n","  ax1.plot(history.history['loss'], label='training')\n","  ax1.plot(history.history['val_loss'], label='validation')\n","  ax1.set_ylabel('Cross-Entropy Loss')\n","  ax1.set_yscale('log')\n","  if history.history.__contains__('lr'):\n","    ax1b = ax1.twinx()\n","    ax1b.plot(history.history['lr'], 'g-', linewidth=1)\n","    ax1b.set_yscale('log')\n","    ax1b.set_ylabel('Learning Rate', color='g')\n","\n","  ax2.plot(history.history['accuracy'], label='training')\n","  ax2.plot(history.history['val_accuracy'], label='validation')\n","  ax2.set_ylabel('Accuracy')\n","  ax2.set_xlabel('Epochs')\n","  ax2.legend()\n","  plt.show() "],"metadata":{"id":"cqXltJE0tTg_","executionInfo":{"status":"ok","timestamp":1642715711648,"user_tz":-60,"elapsed":17,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Defining AGC model"],"metadata":{"id":"7ayKGD7Hq7xe"}},{"cell_type":"markdown","source":["For the implementation of the adaptive gradient clipping we use this following github [repo](https://github.com/sayakpaul/Adaptive-Gradient-Clipping/blob/main/agc.py)"],"metadata":{"id":"3NUeuO_OrNFZ"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def compute_norm(x, axis, keepdims):\n","    return tf.math.reduce_sum(x ** 2, axis=axis, keepdims=keepdims) ** 0.5\n","\n","def unitwise_norm(x):\n","    if len(x.get_shape()) <= 1:  # Scalars and vectors\n","        axis = None\n","        keepdims = False\n","    elif len(x.get_shape()) in [2, 3]:  # Linear layers of shape IO or multihead linear\n","        axis = 0\n","        keepdims = True\n","    elif len(x.get_shape()) == 4:  # Conv kernels of shape HWIO\n","        axis = [0, 1, 2,]\n","        keepdims = True\n","    else:\n","        raise ValueError(f\"Got a parameter with shape not in [1, 2, 4]! {x}\")\n","    return compute_norm(x, axis, keepdims)\n","\n","\n","def adaptive_clip_grad(parameters, gradients, clip_factor=0.01,\n","                       eps=1e-3):\n","    new_grads = []\n","    for (params, grads) in zip(parameters, gradients):\n","        p_norm = unitwise_norm(params)\n","        max_norm = tf.math.maximum(p_norm, eps) * clip_factor\n","        grad_norm = unitwise_norm(grads)\n","        clipped_grad = grads * (max_norm / tf.math.maximum(grad_norm, 1e-6))\n","        new_grad = tf.where(grad_norm < max_norm, grads, clipped_grad)\n","        new_grads.append(new_grad)\n","    return new_grads"],"metadata":{"id":"VtYcO2tTbMtk","executionInfo":{"status":"ok","timestamp":1642715712099,"user_tz":-60,"elapsed":21,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Resnet 152 architecture without batchnorm"],"metadata":{"id":"hofKI_b0rKOe"}},{"cell_type":"code","source":["\"\"\"ResNet152 model for Keras.\n","# Reference:\n","- [Deep Residual Learning for Image Recognition](\n","    https://arxiv.org/abs/1512.03385) (CVPR 2016 Best Paper Award)\n","Adapted from code contributed by BigMoyan.\n","\"\"\"\n","\n","import os\n","import warnings\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","\n","\n","def identity_block(input_tensor, kernel_size, filters, stage, block):\n","    \"\"\"The identity block is the block that has no conv layer at shortcut.\n","    # Arguments\n","        input_tensor: input tensor\n","        kernel_size: default 3, the kernel size of\n","            middle conv layer at main path\n","        filters: list of integers, the filters of 3 conv layer at main path\n","        stage: integer, current stage label, used for generating layer names\n","        block: 'a','b'..., current block label, used for generating layer names\n","    # Returns\n","        Output tensor for the block.\n","    \"\"\"\n","    filters1, filters2, filters3 = filters\n","\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","\n","\n","    x = layers.Conv2D(filters1, (1, 1),\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2a')(input_tensor)\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters2, kernel_size,\n","                      padding='same',\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2b')(x)\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters3, (1, 1),\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2c')(x)\n","\n","    x = layers.add([x, input_tensor])\n","    x = layers.Activation('relu')(x)\n","    return x\n","\n","\n","def conv_block(input_tensor,\n","               kernel_size,\n","               filters,\n","               stage,\n","               block,\n","               strides=(2, 2)):\n","    \"\"\"A block that has a conv layer at shortcut.\n","    # Arguments\n","        input_tensor: input tensor\n","        kernel_size: default 3, the kernel size of\n","            middle conv layer at main path\n","        filters: list of integers, the filters of 3 conv layer at main path\n","        stage: integer, current stage label, used for generating layer names\n","        block: 'a','b'..., current block label, used for generating layer names\n","        strides: Strides for the first conv layer in the block.\n","    # Returns\n","        Output tensor for the block.\n","    Note that from stage 3,\n","    the first conv layer at main path is with strides=(2, 2)\n","    And the shortcut should have strides=(2, 2) as well\n","    \"\"\"\n","    filters1, filters2, filters3 = filters\n","\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","\n","    x = layers.Conv2D(filters1, (1, 1), strides=strides,\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2a')(input_tensor)\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters2, kernel_size, padding='same',\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2b')(x)\n","\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Conv2D(filters3, (1, 1),\n","                      kernel_initializer='he_normal',\n","                      name=conv_name_base + '2c')(x)\n","\n","    shortcut = layers.Conv2D(filters3, (1, 1), strides=strides,\n","                             kernel_initializer='he_normal',\n","                             name=conv_name_base + '1')(input_tensor)\n","\n","    x = layers.add([x, shortcut])\n","    x = layers.Activation('relu')(x)\n","    return x\n","\n","\n","def ResNet152(input_shape=None,\n","             classes=10):\n","    \"\"\"Instantiates the ResNet50 architecture.\n","    # Arguments\n","        input_shape: optional shape tuple, only to be specified\n","            if `include_top` is False (otherwise the input shape\n","            has to be `(224, 224, 3)` (with `channels_last` data format)\n","            or `(3, 224, 224)` (with `channels_first` data format).\n","            It should have exactly 3 inputs channels,\n","            and width and height should be no smaller than 32.\n","            E.g. `(200, 200, 3)` would be one valid value.\n","        classes: optional number of classes to classify images\n","            into, only to be specified if `include_top` is True, and\n","            if no `weights` argument is specified.\n","    # Returns\n","        A Keras model instance.\n","    \"\"\"\n","    \n","    img_input = layers.Input(shape=input_shape)\n","\n","    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n","    x = layers.Conv2D(64, (7, 7),\n","                      strides=(2, 2),\n","                      padding='valid',\n","                      kernel_initializer='he_normal',\n","                      name='conv1')(x)\n","\n","    x = layers.Activation('relu')(x)\n","    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n","    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n","\n","    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n","    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n","    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n","\n","    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n","    for i in range(1,8):\n","      x = identity_block(x, 3, [128, 128, 512], stage=3, block=f\"b_{i}\")\n","\n","    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n","    for i in range(1,36):\n","      x = identity_block(x, 3, [256, 256, 1024], stage=4, block=f\"b_{i}\")\n","\n","    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n","    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n","    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n","\n","    \n","    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n","    x = layers.Dense(classes, activation='softmax', name='fc10')(x)\n","    \n","    model = Model(img_input, x)\n","\n","    return model"],"metadata":{"id":"Pn_A9v9BhdOR","executionInfo":{"status":"ok","timestamp":1642715712103,"user_tz":-60,"elapsed":21,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"1S65Wb-RZhmu","executionInfo":{"status":"ok","timestamp":1642715712104,"user_tz":-60,"elapsed":19,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"outputs":[],"source":["import tensorflow as tf\n","\n","class ResNet152_AGC_Model(Model):\n","\n","  def __init__(self, inputshape, name, threshold=1e-3):\n","        super(ResNet152_AGC_Model, self).__init__()\n","        self.threshold = threshold\n","        self.resnet = ResNet152(input_shape=inputshape, classes=10)\n","\n","  #this bit of code is from keras API\n","  def train_step(self, data):\n","    # Unpack the data\n","    images, y = data\n","\n","    with tf.GradientTape() as tape:\n","        y_pred = self.resnet(images)  # Forward pass\n","        # Compute the loss value\n","        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","\n","    # Compute gradients\n","    trainable_vars = self.resnet.trainable_variables\n","    gradients = tape.gradient(loss, trainable_vars)\n","    # Update gradients\n","    updated_grad = adaptive_clip_grad(trainable_vars, gradients, self.threshold)\n","    # Update weights\n","    self.optimizer.apply_gradients(zip(updated_grad, trainable_vars))\n","    # Update metrics (includes the metric that tracks the loss)\n","    self.compiled_metrics.update_state(y, y_pred)\n","    # Return a dict mapping metric names to current value\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  def test_step(self, data):\n","    images, labels = data\n","    predictions = self.resnet(images, training=False)\n","    loss = self.compiled_loss(labels, predictions)\n","    self.compiled_metrics.update_state(labels, predictions)\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  def call(self, inputs):\n","    return self.resnet(inputs)"]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam\n","clipping_factor = 0.08 # As described in the base paper of this study\n","resnet152 = ResNet152_AGC_Model((32,32,3), threshold=0.08, name='resnet152_agc')\n","\n","resnet152.compile(loss='categorical_crossentropy',\n","            optimizer=Adam(learning_rate=1e-3),\n","            metrics=['accuracy'])"],"metadata":{"id":"2WzmBaw3jtdp","executionInfo":{"status":"ok","timestamp":1642715713505,"user_tz":-60,"elapsed":1416,"user":{"displayName":"Mathieu Jacques","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuqmYelZ_Kv6yga8GgHY07sH9ye4ZTi2sODQXkhQ=s64","userId":"02992467458977810224"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["history = resnet152.fit(train_generator,\n","                        steps_per_epoch=train_steps,# trained with 512\n","                        epochs=100,\n","                        validation_data=test_generator,\n","                        validation_steps=test_steps\n","                       )\n","plot_history(history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ONoiY8lhpG3z","outputId":"d6d33606-4784-4a08-aa7d-12af5865bf5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","98/98 [==============================] - 76s 560ms/step - loss: 20542.1602 - accuracy: 0.1932 - val_loss: 1.8361 - val_accuracy: 0.3064\n","Epoch 2/100\n","98/98 [==============================] - 49s 496ms/step - loss: 1.8364 - accuracy: 0.3171 - val_loss: 1.7128 - val_accuracy: 0.3771\n","Epoch 3/100\n","98/98 [==============================] - 49s 494ms/step - loss: 1.7352 - accuracy: 0.3630 - val_loss: 1.6318 - val_accuracy: 0.4071\n","Epoch 4/100\n","98/98 [==============================] - 49s 497ms/step - loss: 1.6552 - accuracy: 0.3965 - val_loss: 1.5257 - val_accuracy: 0.4435\n","Epoch 5/100\n","98/98 [==============================] - 49s 495ms/step - loss: 1.6177 - accuracy: 0.4145 - val_loss: 1.4705 - val_accuracy: 0.4664\n","Epoch 6/100\n","98/98 [==============================] - 49s 497ms/step - loss: 1.5701 - accuracy: 0.4316 - val_loss: 1.4322 - val_accuracy: 0.4701\n","Epoch 7/100\n","98/98 [==============================] - 49s 496ms/step - loss: 1.5441 - accuracy: 0.4424 - val_loss: 1.3793 - val_accuracy: 0.4940\n","Epoch 8/100\n","98/98 [==============================] - 49s 497ms/step - loss: 1.5165 - accuracy: 0.4546 - val_loss: 1.3990 - val_accuracy: 0.4951\n","Epoch 9/100\n","98/98 [==============================] - 49s 497ms/step - loss: 1.4862 - accuracy: 0.4625 - val_loss: 1.4059 - val_accuracy: 0.4947\n","Epoch 10/100\n","98/98 [==============================] - 49s 498ms/step - loss: 1.4709 - accuracy: 0.4698 - val_loss: 1.3556 - val_accuracy: 0.5101\n","Epoch 11/100\n","98/98 [==============================] - 49s 498ms/step - loss: 1.4380 - accuracy: 0.4843 - val_loss: 1.3158 - val_accuracy: 0.5204\n","Epoch 12/100\n","98/98 [==============================] - 49s 498ms/step - loss: 1.4267 - accuracy: 0.4874 - val_loss: 1.2796 - val_accuracy: 0.5320\n","Epoch 13/100\n","98/98 [==============================] - 49s 497ms/step - loss: 1.3946 - accuracy: 0.5006 - val_loss: 1.3233 - val_accuracy: 0.5125\n","Epoch 14/100\n","98/98 [==============================] - 49s 498ms/step - loss: 1.3823 - accuracy: 0.5036 - val_loss: 1.2784 - val_accuracy: 0.5315\n","Epoch 15/100\n","98/98 [==============================] - 49s 497ms/step - loss: 1.3604 - accuracy: 0.5133 - val_loss: 1.2461 - val_accuracy: 0.5435\n","Epoch 16/100\n","98/98 [==============================] - 49s 495ms/step - loss: 1.3571 - accuracy: 0.5135 - val_loss: 1.2526 - val_accuracy: 0.5482\n","Epoch 17/100\n","98/98 [==============================] - 49s 502ms/step - loss: 1.3229 - accuracy: 0.5280 - val_loss: 1.1938 - val_accuracy: 0.5719\n","Epoch 18/100\n","98/98 [==============================] - 49s 497ms/step - loss: 1.3099 - accuracy: 0.5305 - val_loss: 1.1663 - val_accuracy: 0.5835\n","Epoch 19/100\n","98/98 [==============================] - 49s 495ms/step - loss: 1.3147 - accuracy: 0.5332 - val_loss: 1.1554 - val_accuracy: 0.5896\n","Epoch 20/100\n","98/98 [==============================] - 49s 497ms/step - loss: 1.2881 - accuracy: 0.5414 - val_loss: 1.1772 - val_accuracy: 0.5734\n","Epoch 21/100\n","42/98 [===========>..................] - ETA: 46s - loss: 1.2832 - accuracy: 0.5426"]}]}]}